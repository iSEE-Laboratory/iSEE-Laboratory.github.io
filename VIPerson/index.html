<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VIPerson</title>
    <link rel="preconnect" href="https://rsms.me/">
    <link rel="stylesheet" href="https://rsms.me/inter/inter.css">
    <!-- Using a CDN for Bootstrap for simplicity, assuming local files might not be available -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/css/bootstrap.min.css">
    <!-- Font Awesome for icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <style>
        body {
            background: rgb(255, 255, 255) no-repeat fixed top left;
            font-family: "Inter", 'Open Sans', sans-serif;
        }
        .graph {
            max-width: 100%;
            height: auto;
        }
    </style>
</head>

<body>

    <section>
        <div class="jumbotron text-center mt-0">
            <div class="container-fluid">
                <div class="row">
                    <div class="col">
                        <h2 style="font-size:30px;"><span style="color: #007bff;">VIPerson</span>: Flexibly Generating <span style="color: #007bff;">V</span>irtual <span style="color: #007bff;">I</span>dentity for <span style="color: #007bff;">Person</span> Re-Identification</h2>
                        <h4 style="color:rgb(54, 125, 189);">ICCV 2025</h4>
                        <hr>
                        <p>
                            Xiao-Wen Zhang*¹,³&nbsp; &nbsp;
                            Delong Zhang*¹,³&nbsp; &nbsp;
                            Zhi Ouyang¹,³&nbsp; &nbsp;
                            Jingke Meng¹,³&nbsp; &nbsp;
                            Yi-Xing Peng¹,³&nbsp; &nbsp;
                            <a href="https://www.isee-ai.cn/~zhwshi/">Wei-Shi Zheng</a>¹,²,³<sup>†</sup>
                        </p>
                        <p>
                            <sup>1</sup> Sun Yat-sen University, China &nbsp; &nbsp; <br>
                            <sup>2</sup> Peng Cheng Laboratory, Shenzhen, China <br>
                            <sup>3</sup> Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China <br>
                        </p>
                        <p>
                            * Equal contribution &nbsp; <sup>†</sup> Corresponding author
                        </p>

                        <div class="row justify-content-center">
                            <div class="col-auto">
                                <a class="btn btn-large btn-light" href="VIPerson_ICCV2025 (2).pdf" role="button" target="_blank">
                                    <i class="fa fa-file"></i> Paper
                                </a>
                            </div>
                            <div class="col-auto">
                                <a class="btn btn-large btn-light" href="https://github.com/iSEE-Laboratory/VIPerson" role="button" target="_blank">
                                    <i class="fa fa-github-alt"></i> Code
                                </a>
                            </div>
                            <div class="col-auto">
                                <a class="btn btn-large btn-light" href="https://github.com/iSEE-Laboratory/VIPerson" role="button" target="_blank">
                                    <i class="fa fa-file"></i> Dataset
                                </a>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section>
        <div class="container">
            <div class="row">
                <div class="col-12">
                    <h2 class="text-center"><strong>Abstract</strong></h2>
                    <hr style="margin-top:0px">
                    <p class="text-justify">
                        Person re-identification (ReID) is to match the person images under different camera views. Training ReID models necessitates a substantial amount of labeled real-world data, leading to high labeling costs and privacy issues. Although several ReID data synthetic methods are proposed to address these issues, they fail to generate images with new identities or real-world camera styles. In this paper, we propose a novel pedestrian generation pipeline, VIPerson, to generate camera-realistic pedestrian images with flexible Virtual Identities for the Person ReID task. VIPerson focuses on three key factors in data synthesis: <strong style="color: #007bff;">(I) Virtual identity diversity:</strong> Enhancing the latent diffusion model with our proposed dropout text embedding, we flexibly generate random and hard identities. <strong style="color: #007bff;">(II) Scalable cross-camera variations:</strong> VIPerson introduces scalable variations of scenes and poses within each identity. <strong style="color: #007bff;">(III) Camera-realistic style:</strong> Adopting an identity-agnostic approach to transfer realistic styles, we avoid privacy exposure of real identities. Extensive experimental results across a broad range of downstream ReID tasks demonstrate the superiority of our generated dataset over existing methods. In addition, VIPerson can be adapted to the privacy-constrained ReID scenario, which widens the application of our pipeline.
                    </p>
                    <img src="images/data_generation_pipeline.png" alt="VIPerson Pipeline" class="img-responsive graph d-block mx-auto" width="100%">
                </div>
            </div>
        </div>
    </section>
    <br>

    <section>
        <div class="container">
            <div class="row">
                <div class="col-12">
                    <h2 class="text-center"><strong>Data Examples</strong></h2>
                    <hr style="margin-top:0px">
                    <p class="text-center">Our pipeline can generate diverse virtual identities (random identities and hard identities), and intra-ID groups with rich variations.</p>
                    <h3 class="text-center">Random Identities</h3>
                    <img src="images/Images_of_random_identities.png" alt="Random Identities" class="img-responsive graph d-block mx-auto" width="80%">
                    <br>
                    <h3 class="text-center">Hard Identities</h3>
                    <img src="images/Images_of_hard_identities.png" alt="Hard Identities" class="img-responsive graph d-block mx-auto" width="80%">
                    <br>
                    <h3 class="text-center">Intra-ID Groups (Rich Cross-Camera Variation)</h3>
                    <img src="images/Images_of_intraID_group.png" alt="Intra-ID Groups" class="img-responsive graph d-block mx-auto" width="80%">
                </div>
            </div>
        </div>
    </section>
    <br>

    <section>
        <div class="container">
            <div class="row">
                <div class="col-12">
                    <h2 class="text-center"><strong>Generalize to Downstream ReID Tasks</strong></h2>
                    <hr style="margin-top:0px">
                    <p class="text-justify">
                        We validate our VIPerson dataset on a wide range of downstream ReID tasks, including <b>direct transfer, supervised fine-tuning, and unsupervised domain adaptation</b>. Our generated data consistently outperforms existing synthetic datasets and even real-data baselines in some scenarios, demonstrating its effectiveness and generalization capability.
                    </p>
                    <img src="images/Experiments_of_downstream_reid_task.png" alt="Experimental Results Tables" class="img-responsive graph d-block mx-auto" width="80%">
                    <br>
                    <h3 class="text-center">Analysis of Key Factors in Data synthesis</h3>
                    <p class="text-justify">
                        The figure below illustrates our ablation study on the <b> key components (including poses, scenes, styles) of ReID data synthesis</b>. The results clearly show that removing any single component, such as pose variation  or scene variation , leads to a significant drop in performance on both MSMT17 and Market1501 datasets. The combination of all three factors ('PV+SV+SR') achieves the highest rank-1 accuracy, which validates that all three components are essential and contribute synergistically to the effectiveness of our generated dataset.
                    </p>
                    <div class="row justify-content-center">
                        <div class="col-md-10">
                             <img src="images/Ablation_Study_of_VIPerson.png" alt="Ablation Study Results" class="img-responsive graph d-block mx-auto">
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <br>

    <section>
        <div class="container">
             <div class="row align-items-center mb-4">
                <div class="col-md-6">
                    <img src="images/Effect_of_virtual_identity_diversity.png" alt="Effect of different number of identities" class="img-responsive graph d-block mx-auto" onerror="this.onerror=null;this.src='https://placehold.co/600x400/EEE/31343C?text=Effect+of+Virtual+Identity+Diversity';">
                </div>
                <div class="col-md-6">
                    <h3 class="text-center">Effect of Virtual Identity Diversity</h3>
                    <p class="text-justify">
                        The experimental results show that as the number of identities increases, the model's performance improves, demonstrating that our flexible identity generator (FIG) can generate scalable identities with diverse information. Furthermore, including hard identities allows the ReID model to learn more fine-grained discriminative features, thus improving its ability to distinguish between similar individuals.
                    </p>
                </div>
            </div>
            <div class="row align-items-center">
                <div class="col-md-6">
                     <img src="images/Effect_of_Intra-ID group-scale.png" alt="Effect of different Intra-ID group scales" class="img-responsive graph d-block mx-auto" onerror="this.onerror=null;this.src='https://placehold.co/600x400/EEE/31343C?text=Effect+of+Intra-ID+Group+Scale';">
                </div>
                <div class="col-md-6">
                    <h3 class="text-center">Effect of Intra-ID Group Scale</h3>
                    <p class="text-justify">
                        As the scale of our intra-ID groups increases, performance on both datasets continues to improve. This is thanks to our proposed cross-camera diversification method, which introduces scalable and diverse poses and scenes without manual design, allowing us to expand the intra-ID group scale at a low cost for better performance.
                    </p>
                </div>
            </div>
        </div>
    </section>
    <br>

    <section>
        <div class="container">
            <div class="row ">
                <div class="col-12">
                    <h2><strong>Contact</strong></h2>
                    <hr style="margin-top:0px">
                    <p>If you have any questions, please feel free to comment in github or contact us:
                    <ul>
                        <li><b>Xiao-Wen Zhang</b>: zhangxw75<span style="display:none">Prevent spamming</span>@<span style="display:none">Prevent spamming</span>mail2.sysu.edu.cn</li>
                        <li><b>Delong Zhang</b>: zhangdlong5<span style="display:none">Prevent spamming</span>@<span style="display:none">Prevent spamming</span>mail2.sysu.edu.cn</li>
                    </ul>
                    </p>
                </div>
            </div>
        </div>
    </section>

    <footer class="text-center" style="margin-bottom:10px; font-size: medium;">
        <hr>
        Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the <a href="https://lioryariv.github.io/idr/" target="_blank">website template</a>.
    </footer>

    <script>
        // Fallback for MathJax if needed, though not strictly required by current content
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                macros: {
                    bm: ["{\\boldsymbol #1}", 1],
                }
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</body>

</html>
