<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Grasp as You Say</title>
    <!-- Bootstrap -->
    <link rel="preconnect" href="https://rsms.me/">
    <link rel="stylesheet" href="https://rsms.me/inter/inter.css">
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link href="css/main.css" rel="stylesheet">
    <style>
      body {
        background: rgb(255, 255, 255) no-repeat fixed top left;
        font-family: "Inter", 'Open Sans', sans-serif;
      }
    </style>

  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container-fluid">
        <div class="row">
          <div class="col">
            <h2 style="font-size:30px;">Grasp as You Say: Language-guided Dexterous Grasp Generation</h2>
            <h4 style="color:rgb(54, 125, 189);"> NeurIPS 2024 </h4>
            <hr>
              <a href="https://wyl2077.github.io/"> Yi-Lin Wei</a><sup> 1</sup>&nbsp; &nbsp;
              <a href="https://jianjian-jiang.github.io/">Jian-Jian Jiang</a><sup> 1</sup>&nbsp; &nbsp;
              <a href="https://scholar.google.com/citations?user=BglGZXEAAAAJ&hl=en&oi=ao/">Chengyi Xing</a><sup> 2</sup>&nbsp; &nbsp;
              Xian-Tuo Tan<sup> 1</sup>&nbsp; &nbsp;,
              <a href="https://dravenalg.github.io/">Xiao-Ming Wu</a><sup> 1</sup>&nbsp; &nbsp;
              <a href="https://haolirobo.github.io/">Hao Li</a><sup> 2</sup>&nbsp; &nbsp;
              <a href="https://profiles.stanford.edu/mark-cutkosky">Mark Cutkosky</a><sup> 2</sup>&nbsp; &nbsp;
              <a href="https://www.isee-ai.cn/~zhwshi/">Wei-Shi Zheng</a><sup>1 †</sup>&nbsp; &nbsp;
              <br>
              <br>
            <p>
              <sup>1</sup> Sun Yat-sen University, China &nbsp; &nbsp; <br>
              <sup>2</sup> Stanford University, USA&nbsp; &nbsp; 
              <br>
            </p>
            <p>
              <sup>†</sup> corresponding author &nbsp;
              <br>
            </p>

            <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5">
                    <a class="btn btn-large btn-light" href="https://arxiv.org/abs/2405.19291" role="button" target="_blank">
                    <i class="fa fa-file"></i> Paper </a>
                  </p>
              </div>
              <div class="column">
                <p class="mb-5">
                  <a class="btn btn-large btn-light" href="./index.html" role="button" target="_blank">
                  <i class="fa fa-github-alt"></i> Code (Coming soon) </a>
                </p>
              </div>
              <div class="column">
                <p class="mb-5">
                  <a class="btn btn-large btn-light" href="./index.html" role="button" target="_blank">
                  <i class="fa fa-file"></i> Dataset (Coming soon) </a>
                </p>
            </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="video-section">
    <video class="video-container" src="images/video_demo.mp4" autoplay muted controls>
      Your browser does not support the video tag.
    </video>
  </section>
  
  
  
  <br>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2><strong>Abstract</strong></h2>
            <hr style="margin-top:0px">
              <p class="text-justify" >
                This paper explores a novel task, <strong>"Dexterous Grasp as You Say"</strong>, 
                enabling robots to perform dexterous grasping based on human commands expressed in natural language. 
                However, the development of this field is hindered by the lack of datasets with natural human guidance; 
                thus, we propose a language-guided dexterous grasp dataset, named <strong>DexGYSNet</strong>, 
                offering high-quality dexterous grasp annotations along with flexible and fine-grained human language guidance. 
                Our dataset construction is cost-efficient, with the carefully-design hand-object interaction retargeting strategy, 
                and the LLM-assisted language guidance annotation system. Equipped with this dataset, 
                we introduce the <strong>DexGYSGrasp</strong> framework for generating dexterous grasps based on human language instructions, 
                with the capability of producing grasps that are intent-aligned, high quality and diversity. 
                To achieve this capability, our framework decomposes the complex learning process into two manageable progressive objectives 
                and introduce two components to realize them. The first component learns the grasp distribution focusing on intention alignment 
                and generation diversity. And the second component refines the grasp quality while maintaining intention consistency. 
                Extensive experiments are conducted on DexGYSNet and real world environment for validation.
              </p>
              <div class="row justify-content-center" style="align-items:center; display:flex;"></div>
                <img src="images/task.png" alt="input" class="img-responsive graph" width="100%"/>
              </div>
            <br>
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2><strong>DexGYSNet Construction</strong></h2>
          <hr style="margin-top:0px">
          <p class="text-justify">
            The DexGYSNet dataset is constructed in a cost-effective manner by exploiting human grasp behavior and the extensive capabilities of Large Language Models (LLM).  
            We develop the Hand-Object Interaction Retargeting (HOIR) strategy to transform human grasps into dexterous grasps with high quality and hand-object interaction consistency. 
            Then, we implement an LLM-assisted Language Guidance Annotation system, which leverages the knowledge of Large Language Models (LLM) to produce flexible 
            and fine-grained annotations for language guidance.
          </p>
          <div class="row justify-content-center" style="align-items:center; display:flex;">
            <img src="images/construction.png" alt="input" class="img-responsive graph" width="100%"/>
          </div>
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2><strong>DexGYSGrasp Framework</strong></h2>
          <hr style="margin-top:0px">
          <p class="text-justify">
            DexGYSGrasp framework  decomposes the complex learning task into two sequential objectives managed by progressive components. 
            The first intention and diversity grasp component  learns a grasp distribution by a conditional diffusion. 
            And the second quality grasp component refines the initial coarse grasps to high-quality ones with the same intentions by a transformer.
          </p>
          <div class="row justify-content-center" style="align-items:center; display:flex;">
            <img src="images/methods.png" alt="input" class="img-responsive graph" width="100%"/>
          </div>
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2><strong>Real World Experiment</strong></h2>
          <hr style="margin-top:0px">
          <div class="row">
            <div class="col-md-6">
              <img src="images/realword_setting.png" alt="Real World Experiment Setup" class="img-responsive" width="100%" />
            </div>
            <div class="col-md-6">
              <h3>Experiment Setup</h3>
              <p>
                The real world experiments are conducted on an Allegro hand, a Flexiv Rizon 4 arm, and an Intel Realsense D415 camera. 
                Although our framework is designed for full object point clouds, we integrate several off-the-shelf methods to enhance its practicality. 
                Specifically, partial object point clouds are obtained through visual grounding and SAM, which are then fed into a point cloud completion network 
                to obtain full point clouds. In execution, we first move the arm to the 6-DOF pose of the dexterous hand root node, and then control the 
                dexterous hand joint angles to the predicted poses. Real world experiments further validate the effectiveness of our method.
              </p>
            </div>
            <h3>Experiment Visualization</h3>
            <!-- <div class="row justify-content-center" style="align-items:center; display:flex;">
              <img src="images/sim_vis.png" alt="input" class="img-responsive graph" width="100%"/>
            </div> -->
            <div class="row justify-content-center" style="align-items:center; display:flex;">
              <img src="images/realword_vis.png" alt="input" class="img-responsive graph" width="100%"/>
            </div>
            <div class="row justify-content-center" style="align-items:center; display:flex;">
              <img src="images/realword_process.png" alt="input" class="img-responsive graph" width="100%"/>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Contact -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h2><strong>Contact</strong></h2>
          <hr style="margin-top:0px">
          <p>If you have any questions, please feel free to contact us:
            <ul>
              <li><b>Yi-Lin Wei</b>&colon; weiylin5<span style="display:none">Prevent spamming</span>@<span style="display:none">Prevent spamming</span>mail2.sysu.edu.cn </li>
            </ul>
          </p>
      </div>
    </div>
  </div>


  <footer class="text-center" style="margin-bottom:10px; font-size: medium;">
      <hr>
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the <a href="https://lioryariv.github.io/idr/" target="_blank">website template</a>.
  </footer>
  <script>
    MathJax = {
      tex: {inlineMath: [['$', '$'], ['\\(', '\\)']],
      macros: {
        bm: ["{\\boldsymbol #1}",1],
      }}
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</body>
</html>
